<!DOCTYPE definition [
		<!ENTITY ndash "&#8211;">
<!ENTITY nbsp "&#160;">
<!ENTITY mdash "&#8212;">
<!ENTITY hellip "&#8230;">
<!ENTITY rsquo "&#8217;">
<!ENTITY amp "&#38;">
<!ENTITY lsquo "&#8216;">
<!ENTITY agrave "&#192;">
<!ENTITY ldquo "&#8220;">
<!ENTITY rdquo "&#8221;">
]>
<div id="searchResult" style="visibility: visible;">










	<div id="session_1411" class="resultRow sessionRow ">
		<div class="detailColumn">



			<a href="sessionDetail.ww?SESSION_ID=1411" class="openInPopup">

				<span class="abbreviation">BDT205 - </span>

				<span class="title">Your First Big Data Application on AWS</span>
			</a>


			<span class="abstract truncatedTxt">Want to get ramped up on how to use Amazon's big data web services and launch your first big data application on AWS? Join us on our journey as we build&nbsp;a big data application in real-time using&nbsp;Amazon EMR, Amazon Redshift, Amazon Kinesis, Amazon DynamoDB, and Amazon S3. We review architecture design patterns for big data solutions on AWS, and give you access to a take-home lab so that you can rebuild and customize the application yourself. <a href="javascript:void(0);" class="lessLink">View Less</a></span>


			<small class="length">1 Hour</small>


			<small class="type">Breakout Session</small>


			<span class="track"></span>
				<span class="scheduleStatus">




				</span>
		</div>
		<div class="actionColumn">







		</div>
	</div>


	<div id="session_2807" class="resultRow sessionRow ">
		<div class="detailColumn">



			<a href="sessionDetail.ww?SESSION_ID=2807" class="openInPopup">

				<span class="abbreviation">BDT206 - </span>

				<span class="title">How to Accelerate Your Projects with AWS Marketplace</span>
			</a>


			<span class="abstract truncatedTxt">Learn how Boeing used services from AWS and software from the AWS Marketplace to accelerate the development and launch of analysis software. This session takes you through the end-to-end process of setting up an Amazon Redshift data warehouse, combined with key software from the AWS Marketplace, to help the creation of an analysis tool. This session is ideal for data scientists, technically-inclined business execs, or IT professionals looking to transform their data into new insights. <a href="javascript:void(0);" class="lessLink">View Less</a></span>


			<small class="length">1 Hour</small>


			<small class="type">Breakout Session</small>


			<span class="track"></span>
				<span class="scheduleStatus">




				</span>
		</div>
		<div class="actionColumn">







		</div>
	</div>


	<div id="session_2074" class="resultRow sessionRow ">
		<div class="detailColumn">



			<a href="sessionDetail.ww?SESSION_ID=2074" class="openInPopup">

				<span class="abbreviation">BDT207 - </span>

				<span class="title">Real-Time Analytics In Service of Self-Healing Ecosystems</span>
			</a>


			<span class="abstract truncatedTxt">Netflix strives to provide an amazing experience to each member. To accomplish this, Netflix needs to maintain very high availability across our systems. However, at a certain scale, humans can no longer scale their ability to monitor the status of all systems, making it critical for Netflix to build tools and platforms that can automatically monitor their production environments and make intelligent real-time operational decisions to remedy the problems they identify. In this session, we discuss how Netflix uses data mining and machine learning techniques to automate decisions in real-time with the goal of supporting operational availability, reliability, and consistency. We review how we got to the current states, the lessons we learned, and the future of real-time analytics at Netflix. While Netflix's scale is larger than most other companies, we believe the approaches and technologies we discuss are highly relevant to other production environments, and audience members should come away with actionable ideas that are implementable in, and benefit, most other environments. <a href="javascript:void(0);" class="lessLink">View Less</a></span>


			<small class="length">1 Hour</small>


			<small class="type">Breakout Session</small>


			<span class="track"></span>
				<span class="scheduleStatus">




				</span>
		</div>
		<div class="actionColumn">







		</div>
	</div>


	<div id="session_2874" class="resultRow sessionRow ">
		<div class="detailColumn">



			<a href="sessionDetail.ww?SESSION_ID=2874" class="openInPopup">

				<span class="abbreviation">BDT208 - </span>

				<span class="title">A Technical Introduction to Amazon Elastic MapReduce</span>
			</a>


					<span class="abstract truncatedTxt">Amazon EMR provides a managed framework which makes it easy, cost effective, and secure to run data processing frameworks such as Apache Hadoop, Apache Spark, and Presto on AWS. In this session, you learn the key design principles behind running these frameworks on the cloud and the feature set that Amazon EMR offers. We discuss the benefits of decoupling compute and storage and strategies to take advantage of the scale and the parallelism that the cloud offers, while lowering costs.&nbsp;Additionally, you hear from AOL’s Senior Software Engineer&nbsp;on how they used these strategies to migrate their Hadoop workloads to the AWS cloud and lessons learned along the way.

In this session, you learn the benefits of decoupling storage and compute and allowing them to scale independently; how to run Hadoop, Spark, Presto and other supported Hadoop Applications on Amazon EMR;&nbsp; how to use Amazon S3 as a persistent data-store and process data directly from Amazon S3; dDeployment strategies and how to avoid common mistakes when deploying at scale; and how to use Spot instances to scale your transient infrastructure effectively. <a href="javascript:void(0);" class="lessLink">View Less</a></span>


			<small class="length">1 Hour</small>


			<small class="type">Breakout Session</small>


			<span class="track"></span>
				<span class="scheduleStatus">




				</span>
		</div>
		<div class="actionColumn">







		</div>
	</div>


	<div id="session_2693" class="resultRow sessionRow ">
		<div class="detailColumn">



			<a href="sessionDetail.ww?SESSION_ID=2693" class="openInPopup">

				<span class="abbreviation">BDT209 - </span>

				<span class="title">NEW LAUNCH! Amazon Elasticsearch Service for Real-time Data Analytics and Visualization</span>
			</a>


			<span class="abstract truncatedTxt">Organizations are collecting an ever-increasing amount of data from numerous sources such as log systems, click streams, and connected devices. Launched in 2009, Elasticsearch —an open-source analytics and search engine— has emerged as a popular tool for real-time analytics and visualization of data. Some of the most common use cases include risk assessment, error detection, and sentiment analysis. However, as data volumes and applications grow, managing Elasticsearch clusters can consume significant IT resources while adding little or no differentiated value to the organization. Amazon Elasticsearch Service (Amazon ES) is a managed service that makes it easy to deploy, operate, and scale Elasticsearch clusters in the AWS Cloud. Amazon ES offers the benefits of a managed service, including cluster provisioning, easy configuration, replication for high availability, scaling options, data durability, security, and node monitoring. This session presents a technical deep dive on Amazon ES. Attendees learn: Common challenges with real-time data analytics and visualization and how to address them; the benefits, reference architecture, and best practices for using Amazon ES; and data ingestion options with Amazon DynamoDB, AWS Lambda, and Amazon Kinesis. <a href="javascript:void(0);" class="lessLink">View Less</a></span>


			<small class="length">1 Hour</small>


			<small class="type">Breakout Session</small>


			<span class="track"></span>
				<span class="scheduleStatus">




				</span>
		</div>
		<div class="actionColumn">







		</div>
	</div>


	<div id="session_7046" class="resultRow sessionRow ">
		<div class="detailColumn">



			<a href="sessionDetail.ww?SESSION_ID=7046" class="openInPopup">

				<span class="abbreviation">BDT210 - </span>

				<span class="title">Building Scalable Big Data Solutions: Learn from Intel and AOL How to Solve Enterprise Computational Challenges in the AWS Cloud</span>
			</a>


					<span class="abstract truncatedTxt">Growing data is a massive computational challenge across the enterprise. The opportunity to draw insights from huge data sets is wide open, but traditional computing environments often can’t scale to those volumes. In this session, Intel&nbsp;Chief Data Scientist&nbsp;Bob Rogers PhD&nbsp; explains how developers can take advantage of technologies from Intel with the AWS platform.

Also in this session, AOL Systems Architect Durga Nemani provides insights into how AOL was able to reduce the time and cost to process massive amounts of clickstream data by leveraging big data technologies in AWS.&nbsp; AOL can process data as fast as possible or as cheaply as possible, depending on the SLA, by choosing the number and types of instances without any changes to the code. &nbsp;Session sponsored by Intel. <a href="javascript:void(0);" class="lessLink">View Less</a></span>


			<small class="length">1 Hour</small>


			<small class="type">Breakout Session</small>


			<span class="track"></span>
				<span class="scheduleStatus">




				</span>
		</div>
		<div class="actionColumn">







		</div>
	</div>


	<div id="session_1413" class="resultRow sessionRow ">
		<div class="detailColumn">



			<a href="sessionDetail.ww?SESSION_ID=1413" class="openInPopup">

				<span class="abbreviation">BDT302 - </span>

				<span class="title">Real-World Smart Applications With Amazon Machine Learning</span>
			</a>


			<span class="abstract truncatedTxt">Have you always wanted to add predictive capabilities to your application, but haven’t been able to find the time or the right technology to get started? In this session, learn how an end-to-end smart application can be built in the AWS cloud. We demonstrate how to use Amazon Machine Learning (Amazon&nbsp;ML) to create machine learning models, deploy them to production, and&nbsp;obtain predictions in real-time. We then demonstrate how to build a complete&nbsp;smart&nbsp;application&nbsp;using Amazon ML,&nbsp;Amazon Kinesis, and AWS Lambda. We walk you through the process flow and architecture, demonstrate outcomes, and then dive into the code for implementation. In this session, you learn how to use Amazon&nbsp;ML as well as how to&nbsp;integrate&nbsp;Amazon&nbsp;ML into&nbsp;your&nbsp;applications&nbsp;to take advantage of predictive analysis in the cloud. <a href="javascript:void(0);" class="lessLink">View Less</a></span>


			<small class="length">1 Hour</small>


			<small class="type">Breakout Session</small>


			<span class="track"></span>
				<span class="scheduleStatus">




				</span>
		</div>
		<div class="actionColumn">







		</div>
	</div>


	<div id="session_2080" class="resultRow sessionRow ">
		<div class="detailColumn">



			<a href="sessionDetail.ww?SESSION_ID=2080" class="openInPopup">

				<span class="abbreviation">BDT303 - </span>

				<span class="title">Running Spark and Presto on the Netflix Big Data Platform</span>
			</a>


			<span class="abstract truncatedTxt">In this session, we discuss how Spark and Presto complement the Netflix big data platform stack that started with Hadoop, and the use cases that Spark and Presto address. Also, we discuss how we run Spark and Presto on top of the Amazon EMR infrastructure; specifically, how we use Amazon S3 as our data warehouse and how we leverage Amazon EMR as a generic framework for data-processing cluster management. <a href="javascript:void(0);" class="lessLink">View Less</a></span>


			<small class="length">1 Hour</small>


			<small class="type">Breakout Session</small>


			<span class="track"></span>
				<span class="scheduleStatus">




				</span>
		</div>
		<div class="actionColumn">







		</div>
	</div>


	<div id="session_1484" class="resultRow sessionRow ">
		<div class="detailColumn">



			<a href="sessionDetail.ww?SESSION_ID=1484" class="openInPopup">

				<span class="abbreviation">BDT305 - </span>

				<span class="title">Amazon EMR Deep Dive and Best Practices</span>
			</a>


			<span class="abstract truncatedTxt">Amazon EMR is one of the largest Hadoop operators in the world. In this session, we introduce you to Amazon EMR design patterns such as using Amazon S3 instead of HDFS, taking advantage of both long and short-lived clusters, and other Amazon EMR architectural best practices. We talk about how to scale your cluster up or down dynamically and introduce you to ways you can fine-tune your cluster. We will also share best practices to keep your Amazon EMR cluster cost-efficient. Finally, we dive into some of our recent launches to keep you current on our latest features. <a href="javascript:void(0);" class="lessLink">View Less</a></span>


			<small class="length">1 Hour</small>


			<small class="type">Breakout Session</small>


			<span class="track"></span>
				<span class="scheduleStatus">




				</span>
		</div>
		<div class="actionColumn">







		</div>
	</div>


	<div id="session_1608" class="resultRow sessionRow ">
		<div class="detailColumn">



			<a href="sessionDetail.ww?SESSION_ID=1608" class="openInPopup">

				<span class="abbreviation">BDT306 - </span>

				<span class="title">The Life of a Click: How Hearst Publishing Manages Clickstream Analytics with AWS</span>
			</a>


			<span class="abstract truncatedTxt">Hearst Corporation monitors trending content on 250+ sites worldwide, providing metrics to editors and promoting cross-platform content sharing. To facilitate this, Hearst built a clickstream analytics platform on AWS that transmits and processes over 30 TB of data a day using AWS resources such as AWS Elastic Beanstalk, Amazon Kinesis, Spark on Amazon EMR, Amazon S3, Amazon Redshift, and Amazon Elasticsearch. In this session, learn how Hearst designed their clickstream analytics application and how you can use the same architecture to build your own and be ready to handle the changing world of clickstream data. Dive into how to do Spark streaming from an Amazon Kinesis stream, use timestamps to cleanse and validate data coming from diverse sources, and see how the system has evolved as data types have change from HTTP GET to RESTful JSON requests. Finally, see how Hearst's data scientists interact with and use cleansed data provided by the platform to perform ad hoc analyses, develop home-grown algorithms, and create visualizations and dashboards that support Hearst business stakeholders. <a href="javascript:void(0);" class="lessLink">View Less</a></span>


			<small class="length">1 Hour</small>


			<small class="type">Breakout Session</small>


			<span class="track"></span>
				<span class="scheduleStatus">




				</span>
		</div>
		<div class="actionColumn">







		</div>
	</div>


	<div id="session_2339" class="resultRow sessionRow ">
		<div class="detailColumn">



			<a href="sessionDetail.ww?SESSION_ID=2339" class="openInPopup">

				<span class="abbreviation">BDT307 - </span>

				<span class="title">Zero Infrastructure, Real-Time Data Collection, and Analytics</span>
			</a>


			<span class="abstract truncatedTxt">Any fast-growing organization needs a way to manage the ever-increasing volume of data being generated across the globe and the need for real-time analysis. In this session, we walk through a real-life architecture and demonstration of how to leverage Amazon Kinesis, AWS Lambda, Amazon S3, and Amazon Redshift/Aurora for near real-time access to data being collected around the globe. We dive deep into performance, cost, and system resiliency and give you practical tools you can use today to manage your own global data ingestion pipeline and produce quality analytics in real-time without building infrastructure. <a href="javascript:void(0);" class="lessLink">View Less</a></span>


			<small class="length">1 Hour</small>


			<small class="type">Breakout Session</small>


			<span class="track"></span>
				<span class="scheduleStatus">




				</span>
		</div>
		<div class="actionColumn">







		</div>
	</div>


	<div id="session_2976" class="resultRow sessionRow ">
		<div class="detailColumn">



			<a href="sessionDetail.ww?SESSION_ID=2976" class="openInPopup">

				<span class="abbreviation">BDT309 - </span>

				<span class="title">Data Science &amp; Best Practices for Apache Spark on Amazon EMR</span>
			</a>


			<span class="abstract truncatedTxt">Organizations need to perform increasingly complex analysis on their data&nbsp;—&nbsp;streaming analytics, ad-hoc querying and predictive analytics&nbsp;—&nbsp;in order to get better customer insights and actionable business intelligence. However, the growing data volume, speed, and complexity of diverse data formats make current tools inadequate or difficult to use. Apache Spark has recently emerged as the framework of choice to address these challenges. Spark is a general-purpose processing framework that follows a DAG model and also provides high-level APIs, making it more flexible and easier to use than MapReduce. Thanks to its use of in-memory datasets (RDDs), embedded libraries, fault-tolerance, and support for a variety of programming languages, Apache Spark enables developers to implement and scale far more complex big data use cases, including real-time data processing, interactive querying, graph computations and predictive analytics. In this session, we present a technical deep dive on Spark running on Amazon EMR. You learn why Spark is great for ad-hoc interactive analysis and real-time stream processing, how to deploy and tune scalable clusters running Spark on Amazon EMR, how to use EMRFS with Spark to query data directly in Amazon S3, and best practices and patterns for Spark on Amazon EMR. <a href="javascript:void(0);" class="lessLink">View Less</a></span>


			<small class="length">1 Hour</small>


			<small class="type">Breakout Session</small>


			<span class="track"></span>
				<span class="scheduleStatus">




				</span>
		</div>
		<div class="actionColumn">







		</div>
	</div>


	<div id="session_1409" class="resultRow sessionRow ">
		<div class="detailColumn">



			<a href="sessionDetail.ww?SESSION_ID=1409" class="openInPopup">

				<span class="abbreviation">BDT310 - </span>

				<span class="title">Big Data Architectural Patterns and Best Practices on AWS</span>
			</a>


			<span class="abstract truncatedTxt">The world is producing an ever increasing volume, velocity, and variety of big data. Consumers and businesses are demanding up-to-the-second (or even millisecond) analytics on their fast-moving data, in addition to classic batch processing. AWS delivers many technologies for solving big data problems. But what services should you use, why, when, and how? In this session, we simplify big data processing as a data bus comprising various stages: ingest, store, process, and visualize. Next, we discuss how to choose the right technology in each stage based on criteria such as data structure, query latency, cost, request rate, item size, data volume, durability, and so on. Finally, we provide reference architecture, design patterns, and best practices for assembling these technologies to solve your big data problems&nbsp;at the right cost. <a href="javascript:void(0);" class="lessLink">View Less</a></span>


			<small class="length">1 Hour</small>


			<small class="type">Breakout Session</small>


			<span class="track"></span>
				<span class="scheduleStatus">




				</span>
		</div>
		<div class="actionColumn">







		</div>
	</div>


	<div id="session_6635" class="resultRow sessionRow ">
		<div class="detailColumn">



			<a href="sessionDetail.ww?SESSION_ID=6635" class="openInPopup">

				<span class="abbreviation">BDT310-APAC - </span>

				<span class="title">Big Data Architectural Patterns and Best Practices on AWS (APAC track)</span>
			</a>


					<span class="abstract truncatedTxt">The world is producing an ever-increasing volume, velocity, and variety of big data. Consumers and businesses are demanding up-to-the-second (or even millisecond) analytics on their fast-moving data, in addition to classic batch processing. AWS delivers many technologies for solving big data problems. But what services should you use, why, when, and how? In this session, we simplify big data processing as a data bus comprising various stages: ingest, store, process, and visualize. Next, we discuss how to choose the right technology in each stage based on criteria such as data structure, query latency, cost, request rate, item size, data volume, durability, and so on. Finally, we provide reference architecture, design patterns, and best practices for assembling these technologies to solve your big data problems at the right cost.

This is a repeat session that will be translated simultaneously into Japanese, Chinese, and Korean. <a href="javascript:void(0);" class="lessLink">View Less</a></span>


			<small class="length">1 Hour</small>


			<small class="type">Breakout Session</small>


			<span class="track"></span>
				<span class="scheduleStatus">




				</span>
		</div>
		<div class="actionColumn">







		</div>
	</div>


	<div id="session_4756" class="resultRow sessionRow ">
		<div class="detailColumn">



			<a href="sessionDetail.ww?SESSION_ID=4756" class="openInPopup">

				<span class="abbreviation">BDT311 - </span>

				<span class="title">Deep Learning: Going Beyond Machine Learning</span>
			</a>


			<span class="abstract truncatedTxt">Deep learning goes beyond the traditional machine learning of big data and analytics. In this session, we will review the AWS offering, Amazon Machine Learning, and the AWS GPU-intensive family of servers that run native machine learning and deep-learning algorithms. We will also cover some basic deep-learning algorithms using open source software.&nbsp; Session sponsored by Day1 Solutions. <a href="javascript:void(0);" class="lessLink">View Less</a></span>


			<small class="length">1 Hour</small>


			<small class="type">Breakout Session</small>


			<span class="track"></span>
				<span class="scheduleStatus">




				</span>
		</div>
		<div class="actionColumn">







		</div>
	</div>


	<div id="session_4839" class="resultRow sessionRow ">
		<div class="detailColumn">



			<a href="sessionDetail.ww?SESSION_ID=4839" class="openInPopup">

				<span class="abbreviation">BDT312 - </span>

				<span class="title">Application Monitoring in a Post-Server World: Why Data Context Is Critical</span>
			</a>


					<span class="abstract truncatedTxt">The move towards microservices in Docker, EC2 and Lambda points to a shift towards shorter lived resources. These new application architectures are driving new agility and efficiency. But they, while providing developers with inherent scalability, elasticity, and flexibility, also present new challenges for application monitoring. The days of static server monitoring with a single health and status check are over. These days you need to know how your entire ecosystem of AWS EC2 instances are performing, especially since many of them are short lived and may only exist for a few minutes. With such ephemeral resources, there is no server to monitor; you need to understand performance along the lines of computation intent. And for this, you need the context in which these resources are performing.

Join Kevin McGuire, Director of Engineering at New Relic, as he discusses trends in computing that we’ve gleaned from monitoring Docker and how they’ve helped us rethink how we monitor and analyze AWS. We’ll dive into the case for how contextual information like instance size, AMI, availability zone and tags can be used to drive an elevated understanding of transient infrastructure behavior and how it contributes to application performance. We’ll show how integrating status information gives you a more accurate view of EC2 lifecycle and health. And finally, how that information powers the ability for you to analyze and display that performance information in new and powerful ways. <a href="javascript:void(0);" class="lessLink">View Less</a></span>


			<small class="length">1 Hour</small>


			<small class="type">Breakout Session</small>


			<span class="track"></span>
				<span class="scheduleStatus">




				</span>
		</div>
		<div class="actionColumn">







		</div>
	</div>


	<div id="session_2978" class="resultRow sessionRow ">
		<div class="detailColumn">



			<a href="sessionDetail.ww?SESSION_ID=2978" class="openInPopup">

				<span class="abbreviation">BDT313 - </span>

				<span class="title">Amazon DynamoDB for Big Data</span>
			</a>


			<span class="abstract truncatedTxt">NoSQL is an important part of many big data strategies. Attend this session to learn how Amazon DynamoDB helps you create fast ingest and response data sets. We demonstrate how to use DynamoDB for batch-based query processing and ETL operations (using a SQL-like language) through integration with Amazon EMR and Hive. Then, we show you how to reduce costs and achieve scalability by connecting data to Amazon ElasticCache for handling massive read volumes. We’ll also discuss how to add indexes on DynamoDB data for free-text searching by integrating with Elasticsearch using AWS Lambda and DynamoDB streams. Finally, you’ll find out how you can take your high-velocity, high-volume data (such as IoT data) in DynamoDB and connect it to a data warehouse (Amazon Redshift) to enable BI analysis. <a href="javascript:void(0);" class="lessLink">View Less</a></span>


			<small class="length">1 Hour</small>


			<small class="type">Breakout Session</small>


			<span class="track"></span>
				<span class="scheduleStatus">




				</span>
		</div>
		<div class="actionColumn">







		</div>
	</div>


	<div id="session_2778" class="resultRow sessionRow ">
		<div class="detailColumn">



			<a href="sessionDetail.ww?SESSION_ID=2778" class="openInPopup">

				<span class="abbreviation">BDT314 - </span>

				<span class="title">Running a Big Data and Analytics Application on Amazon EMR and Amazon Redshift with a Focus on Security</span>
			</a>


					<span class="abstract truncatedTxt">No matter the industry, leading organizations need to closely integrate, deploy, secure, and scale diverse technologies to support workloads while containing costs. Nasdaq, Inc.—a leading provider of trading, clearing, and exchange technology—is no exception.

After migrating more than 1,100 tables from a legacy data warehouse into Amazon Redshift, Nasdaq, Inc. is now implementing a fully-integrated, big data architecture that also includes Amazon S3, Amazon EMR, and Presto to securely analyze large historical data sets in a highly regulated environment. Drawing from this experience, Nasdaq, Inc. shares lessons learned and best practices for deploying a highly secure, unified, big data architecture on AWS.

Attendees learn:

Architectural recommendations to extend an Amazon Redshift data warehouse with Amazon EMR and Presto.

Tips to migrate historical data from an on-premises solution and Amazon Redshift to Amazon S3, making it consumable.

Best practices for securing critical data and applications leveraging encryption, SELinux, and VPC. <a href="javascript:void(0);" class="lessLink">View Less</a></span>


			<small class="length">1 Hour</small>


			<small class="type">Breakout Session</small>


			<span class="track"></span>
				<span class="scheduleStatus">




				</span>
		</div>
		<div class="actionColumn">







		</div>
	</div>


	<div id="session_3171" class="resultRow sessionRow ">
		<div class="detailColumn">



			<a href="sessionDetail.ww?SESSION_ID=3171" class="openInPopup">

				<span class="abbreviation">BDT316 - </span>

				<span class="title">Offloading ETL to Amazon Elastic MapReduce</span>
			</a>


					<span class="abstract truncatedTxt">Amgen discovers, develops, manufactures, and delivers innovative human therapeutics, helping millions of people in the fight against serious illnesses. In 2014, Amgen implemented a solution to offload ETL data across a diverse data set (U.S. pharmaceutical prescriptions and claims) using Amazon EMR. The solution has transformed the way Amgen delivers insights and reports to its sales force. To support Amgen’s entry into a much larger market, the ETL process had to scale to eight times its existing data volume. We used&nbsp;Amazon EC2, Amazon S3, Amazon EMR, and&nbsp;Amazon Redshift&nbsp;to generate weekly sales reporting metrics.

This session discusses highlights in Amgen's journey to leverage big data technologies and lay the foundation for future growth: benefits of ETL offloading in Amazon EMR as an entry point for big data technologies; benefits and challenges of using Amazon EMR vs. expanding on-premises ETL and reporting technologies; and how to architect an ETL offload solution using Amazon S3, Amazon EMR, and Impala. <a href="javascript:void(0);" class="lessLink">View Less</a></span>


			<small class="length">1 Hour</small>


			<small class="type">Breakout Session</small>


			<span class="track"></span>
				<span class="scheduleStatus">




				</span>
		</div>
		<div class="actionColumn">







		</div>
	</div>


	<div id="session_2980" class="resultRow sessionRow ">
		<div class="detailColumn">



			<a href="sessionDetail.ww?SESSION_ID=2980" class="openInPopup">

				<span class="abbreviation">BDT317 - </span>

				<span class="title">Building a Data Lake on AWS</span>
			</a>


					<span class="abstract truncatedTxt">Conceptually, a data lake is a flat data store to collect data in its original form, without the need to enforce a predefined schema. Instead, new schemas or views are created “on demand”, providing a far more agile and flexible architecture while enabling new types of analytical insights. AWS provides many of the building blocks required to help organizations implement a data lake. In this session, we will introduce key concepts for a data lake and present aspects related to its implementation. We will discuss critical success factors, pitfalls to avoid as well as operational aspects such as security, governance, search, indexing and metadata management. We will also provide insight on how AWS enables a data lake architecture. &nbsp;

A data lake is a flat data store to collect data in its original form, without the need to enforce a predefined schema. Instead, new schemas or views are created "on demand", providing a far more agile and flexible architecture while enabling new types of analytical insights. AWS provides many of the building blocks required to help organizations implement a data lake. In this session, we introduce key concepts for a data lake and present aspects related to its implementation. We discuss critical success factors and pitfalls to avoid, as well as operational aspects such as security, governance, search, indexing, and metadata management. We also provide insight on how AWS enables a data lake architecture. Attendees get practical tips and recommendations to get started with their data lake implementations on AWS. <a href="javascript:void(0);" class="lessLink">View Less</a></span>


			<small class="length">1 Hour</small>


			<small class="type">Breakout Session</small>


			<span class="track"></span>
				<span class="scheduleStatus">




				</span>
		</div>
		<div class="actionColumn">







		</div>
	</div>


	<div id="session_2095" class="resultRow sessionRow ">
		<div class="detailColumn">



			<a href="sessionDetail.ww?SESSION_ID=2095" class="openInPopup">

				<span class="abbreviation">BDT318 - </span>

				<span class="title">Netflix Keystone: How Netflix Handles Data Streams Up to 8 Million Events Per Second</span>
			</a>


			<span class="abstract truncatedTxt">In this session, Netflix provides an overview of Keystone, their new data pipeline. The session covers how Netflix migrated from Suro to Keystone, including the reasons behind the transition and the challenges of zero loss while processing over 400 billion events daily. The session covers in detail how they deploy, operate, and scale Kafka, Samza, Docker, and Apache Mesos in AWS to manage 8 million events &amp; 17 GB per second during peak. <a href="javascript:void(0);" class="lessLink">View Less</a></span>


			<small class="length">1 Hour</small>


			<small class="type">Breakout Session</small>


			<span class="track"></span>
				<span class="scheduleStatus">




				</span>
		</div>
		<div class="actionColumn">







		</div>
	</div>


	<div id="session_2973" class="resultRow sessionRow ">
		<div class="detailColumn">



			<a href="sessionDetail.ww?SESSION_ID=2973" class="openInPopup">

				<span class="abbreviation">BDT319 - </span>

				<span class="title">NEW LAUNCH! Amazon QuickSight: Very Fast, Easy-to-Use, Cloud-native Business Intelligence</span>
			</a>

			<label style="color:#0126f5;">Just Added!</label>


			<span class="abstract truncatedTxt">Amazon&nbsp;QuickSight&nbsp;is a&nbsp;very fast, cloud-powered business intelligence (BI) service that makes it easy to build visualizations, perform ad-hoc analysis, and quickly get business insights from your data.&nbsp;In this session, we demonstrate how you can point Amazon&nbsp;QuickSight&nbsp;to AWS data stores, flat files, or other third-party data sources and begin visualizing your data in minutes. We also introduce SPICE - &nbsp;a new Super-fast, Parallel, In-memory, Calculation Engine in Amazon&nbsp;QuickSight, which&nbsp;performs advanced calculations and render visualizations rapidly&nbsp;without requiring any additional infrastructure, SQL programming, or dimensional modeling, so you can seamlessly scale to hundreds of thousands of users and petabytes of data. Lastly, you will see how Amazon&nbsp;QuickSight&nbsp;provides you with smart visualizations and graphs that are optimized for your different data types, to ensure the most suitable and appropriate visualization to conduct your analysis, and how to share these visualization stories using the built-in collaboration tools. <a href="javascript:void(0);" class="lessLink">View Less</a></span>


			<small class="length">1 Hour</small>


			<small class="type">Breakout Session</small>


			<span class="track"></span>
				<span class="scheduleStatus">




				</span>
		</div>
		<div class="actionColumn">







		</div>
	</div>


	<div id="session_7111" class="resultRow sessionRow ">
		<div class="detailColumn">



			<a href="sessionDetail.ww?SESSION_ID=7111" class="openInPopup">

				<span class="abbreviation">BDT319-R - </span>

				<span class="title">NEW LAUNCH! Amazon QuickSight: Very Fast, Easy-to-Use, Cloud-native Business Intelligence (Repeat Session)</span>
			</a>

			<label style="color:#0126f5;">Just Added!</label>


			<span class="abstract truncatedTxt">Amazon&nbsp;QuickSight&nbsp;is a&nbsp;very fast, cloud-powered business intelligence (BI) service that makes it easy to build visualizations, perform ad-hoc analysis, and quickly get business insights from your data.&nbsp;In this session, we demonstrate how you can point Amazon&nbsp;QuickSight&nbsp;to AWS data stores, flat files, or other third-party data sources and begin visualizing your data in minutes. We also introduce SPICE - &nbsp;a new Super-fast, Parallel, In-memory, Calculation Engine in Amazon&nbsp;QuickSight, which&nbsp;performs advanced calculations and render visualizations rapidly&nbsp;without requiring any additional infrastructure, SQL programming, or dimensional modeling, so you can seamlessly scale to hundreds of thousands of users and petabytes of data. Lastly, you will see how Amazon&nbsp;QuickSight&nbsp;provides you with smart visualizations and graphs that are optimized for your different data types, to ensure the most suitable and appropriate visualization to conduct your analysis, and how to share these visualization stories using the built-in collaboration tools. <a href="javascript:void(0);" class="lessLink">View Less</a></span>


			<small class="length">1 Hour</small>


			<small class="type">Breakout Session</small>


			<span class="track"></span>
				<span class="scheduleStatus">




				</span>
		</div>
		<div class="actionColumn">







		</div>
	</div>


	<div id="session_7326" class="resultRow sessionRow ">
		<div class="detailColumn">



			<a href="sessionDetail.ww?SESSION_ID=7326" class="openInPopup">

				<span class="abbreviation">BDT319-RR - </span>

				<span class="title">NEW LAUNCH!! Amazon QuickSight: Very Fast, Easy-to-Use, Cloud-native Business Intelligence (Repeat Session)</span>
			</a>

			<label style="color:#0126f5;">Just Added!</label>


			<span class="abstract truncatedTxt">Amazon&nbsp;QuickSight&nbsp;is a&nbsp;very fast, cloud-powered business intelligence (BI) service that makes it easy to build visualizations, perform ad-hoc analysis, and quickly get business insights from your data.&nbsp;In this session, we demonstrate how you can point Amazon&nbsp;QuickSight&nbsp;to AWS data stores, flat files, or other third-party data sources and begin visualizing your data in minutes. We also introduce SPICE - &nbsp;a new Super-fast, Parallel, In-memory, Calculation Engine in Amazon&nbsp;QuickSight, which&nbsp;performs advanced calculations and render visualizations rapidly&nbsp;without requiring any additional infrastructure, SQL programming, or dimensional modeling, so you can seamlessly scale to hundreds of thousands of users and petabytes of data. Lastly, you will see how Amazon&nbsp;QuickSight&nbsp;provides you with smart visualizations and graphs that are optimized for your different data types, to ensure the most suitable and appropriate visualization to conduct your analysis, and how to share these visualization stories using the built-in collaboration tools. <a href="javascript:void(0);" class="lessLink">View Less</a></span>


			<small class="length">1 Hour</small>


			<small class="type">Breakout Session</small>


			<span class="track"></span>
				<span class="scheduleStatus">




				</span>
		</div>
		<div class="actionColumn">







		</div>
	</div>


	<div id="session_3728" class="resultRow sessionRow ">
		<div class="detailColumn">



			<a href="sessionDetail.ww?SESSION_ID=3728" class="openInPopup">

				<span class="abbreviation">BDT320 - </span>

				<span class="title">NEW LAUNCH! Streaming Data Flows with Amazon Kinesis Firehose and Amazon Kinesis Analytics</span>
			</a>

			<label style="color:#0126f5;">Just Added!</label>


			<span class="abstract truncatedTxt">Amazon Kinesis&nbsp;Firehose is a fully-managed, elastic service to deliver real-time data streams to Amazon S3, Amazon Redshift, and other destinations. In this session, we start with overviews of Amazon Kinesis Firehose and Amazon Kinesis Analytics. We then discuss how Amazon Kinesis&nbsp;Firehose makes it even easier to get started with streaming data, without writing a stream processing application or provisioning a single resource. You learn about the key features of Amazon Kinesis&nbsp;Firehose, including its companion agent that makes emitting data from data producers even easier. We walk through capture and delivery with an end-to-end demo, and discuss key metrics that will help developers and architects understand their streaming data flow. Finally, we look at some patterns for data consumption as the data streams into S3. We show two examples: using AWS Lambda, and how you can use Apache Spark running within Amazon EMR to query data directly in Amazon S3 through EMRFS. <a href="javascript:void(0);" class="lessLink">View Less</a></span>


			<small class="length">1 Hour</small>


			<small class="type">Breakout Session</small>


			<span class="track"></span>
				<span class="scheduleStatus">




				</span>
		</div>
		<div class="actionColumn">







		</div>
	</div>


	<div id="session_3371" class="resultRow sessionRow ">
		<div class="detailColumn">



			<a href="sessionDetail.ww?SESSION_ID=3371" class="openInPopup">

				<span class="abbreviation">BDT322 - </span>

				<span class="title">How Redfin and Twitter Leverage Amazon S3 to Build Their Big Data Platforms</span>
			</a>


			<span class="abstract truncatedTxt">Analyzing large data sets requires significant compute and storage capacity that can vary in size based on the amount of input data and the analysis required. This characteristic of big data workloads is ideally suited to the pay-as-you-go cloud model, where applications can easily scale up and down based on demand. Learn how Amazon S3 can help scale your big data platform. Hear from Redfin and Twitter about how they build their big data platforms on AWS and how they use S3 as an integral piece of their big data platforms. <a href="javascript:void(0);" class="lessLink">View Less</a></span>


			<small class="length">1 Hour</small>


			<small class="type">Breakout Session</small>


			<span class="track"></span>
				<span class="scheduleStatus">




				</span>
		</div>
		<div class="actionColumn">







		</div>
	</div>


	<div id="session_2984" class="resultRow sessionRow ">
		<div class="detailColumn">



			<a href="sessionDetail.ww?SESSION_ID=2984" class="openInPopup">

				<span class="abbreviation">BDT323 - </span>

				<span class="title">Amazon EBS and Cassandra: 1 Million Writes Per Second on 60 Nodes</span>
			</a>


			<span class="abstract truncatedTxt">With the introduction of Amazon Elastic Block Store (EBS) GP2 and recent stability improvements, EBS has gained credibility in the Cassandra world for high performance workloads. By running Cassandra on Amazon EBS, you can run denser, cheaper Cassandra clusters with just as much availability as ephemeral storage instances. This talk walks through a highly detailed use case and configuration guide for a multi PetaByte, million write per second cluster that needs to be high performing and cost efficient. We explore the instance type choices, configuration, and low-level tuning that allowed us to hit 1.3 million writes per second with a replication factor of 3 on just 60 nodes. <a href="javascript:void(0);" class="lessLink">View Less</a></span>


			<small class="length">1 Hour</small>


			<small class="type">Breakout Session</small>


			<span class="track"></span>
				<span class="scheduleStatus">




				</span>
		</div>
		<div class="actionColumn">







		</div>
	</div>


	<div id="session_6976" class="resultRow sessionRow ">
		<div class="detailColumn">



			<a href="sessionDetail.ww?SESSION_ID=6976" class="openInPopup">

				<span class="abbreviation">BDT324 - </span>

				<span class="title">Big Data Optimized for the AWS Cloud</span>
			</a>


			<span class="abstract truncatedTxt">Apache Hadoop is now a foundational platform for big data processing and discovery that drives next-generation analytics. While Hadoop was designed when cloud models were in their infancy, the open source platform works remarkably well in production environments in the cloud. This talk will cover use cases for running big data in the cloud and share examples of organizations that have experienced real-world success on AWS. We will also look at new software and hardware innovations that are helping companies get more value from their data.&nbsp;Session sponsored by Intel. <a href="javascript:void(0);" class="lessLink">View Less</a></span>


			<small class="length">1 Hour</small>


			<small class="type">Breakout Session</small>


			<span class="track"></span>
				<span class="scheduleStatus">




				</span>
		</div>
		<div class="actionColumn">







		</div>
	</div>


	<div id="session_3735" class="resultRow sessionRow ">
		<div class="detailColumn">



			<a href="sessionDetail.ww?SESSION_ID=3735" class="openInPopup">

				<span class="abbreviation">BDT401 - </span>

				<span class="title">Amazon Redshift Deep Dive: Tuning and Best Practices</span>
			</a>


			<span class="abstract truncatedTxt">Get a look under the covers: Learn tuning best practices for taking advantage of Amazon Redshift's columnar technology and parallel processing capabilities to improve your delivery of queries and improve overall database performance. This session explains how to migrate from existing data warehouses, create an optimized schema, efficiently load data, use work load management, tune your queries, and use Amazon Redshift's interleaved sorting features. Finally, learn how TripAdvisor uses&nbsp;these best practices to give their entire organization access to analytic insights at scale.&nbsp; <a href="javascript:void(0);" class="lessLink">View Less</a></span>


			<small class="length">1 Hour</small>


			<small class="type">Breakout Session</small>


			<span class="track"></span>
				<span class="scheduleStatus">




				</span>
		</div>
		<div class="actionColumn">







		</div>
	</div>


	<div id="session_4804" class="resultRow sessionRow ">
		<div class="detailColumn">



			<a href="sessionDetail.ww?SESSION_ID=4804" class="openInPopup">

				<span class="abbreviation">BDT402 - </span>

				<span class="title">Delivering Business Agility Using AWS</span>
			</a>


					<span class="abstract truncatedTxt">Wipro is one of India's largest publicly traded companies and the seventh largest IT services firm in the world. In this session, we showcase the structured methods that Wipro has used in enabling enterprises to take advantage of the cloud. These cover identifying workloads and application profiles that could benefit, re-structuring enterprise application and infrastructure components for migration, rapid and thorough verification and validation, and modifying component monitoring and management.

Several of these methods can be tailored to the individual client or functional context, so specific client examples are presented. We also discuss the enterprise experience of enabling many non-IT functions to benefit from the cloud, such as sales and training. More functions included in the cloud increase the benefit drawn from a cloud-enabled IT landscape.

Session sponsored by Wipro. <a href="javascript:void(0);" class="lessLink">View Less</a></span>


			<small class="length">1 Hour</small>


			<small class="type">Breakout Session</small>


			<span class="track"></span>
				<span class="scheduleStatus">




				</span>
		</div>
		<div class="actionColumn">







		</div>
	</div>


	<div id="session_2949" class="resultRow sessionRow ">
		<div class="detailColumn">



			<a href="sessionDetail.ww?SESSION_ID=2949" class="openInPopup">

				<span class="abbreviation">BDT403 - </span>

				<span class="title">Best Practices for Building Real-time Streaming Applications with Amazon Kinesis</span>
			</a>


			<span class="abstract truncatedTxt">Amazon Kinesis is a fully managed, cloud-based service for real-time data processing over large, distributed data streams. Customers who use Amazon Kinesis can continuously capture and process real-time data such as website clickstreams, financial transactions, social media feeds, IT logs, location-tracking events, and more. In this session, we first focus on building a scalable, durable streaming data ingest workflow, from data producers like mobile devices, servers, or even a web browser, using the right tool for the right job. Then, we cover code design that minimizes duplicates and achieves exactly-once processing semantics in your elastic stream-processing application, built with the Kinesis Client Library. Attend this session to learn best practices for building a real-time streaming data architecture with Amazon Kinesis, and get answers to technical questions frequently asked by those starting to process streaming events. <a href="javascript:void(0);" class="lessLink">View Less</a></span>


			<small class="length">1 Hour</small>


			<small class="type">Breakout Session</small>


			<span class="track"></span>
				<span class="scheduleStatus">




				</span>
		</div>
		<div class="actionColumn">







		</div>
	</div>


	<div id="session_2384" class="resultRow sessionRow ">
		<div class="detailColumn">



			<a href="sessionDetail.ww?SESSION_ID=2384" class="openInPopup">

				<span class="abbreviation">BDT404 - </span>

				<span class="title">Building and Managing Large-Scale ETL Data Flows with AWS Data Pipeline and Dataduct</span>
			</a>


					<span class="abstract truncatedTxt">As data volumes grow, managing and scaling data pipelines for ETL and batch processing can be daunting. With more than 13.5 million learners worldwide, hundreds of courses, and thousands of instructors, Coursera manages over a hundred data pipelines for ETL, batch processing, and new product development.

In this session, we dive deep into AWS Data Pipeline and Dataduct, an open source framework built at Coursera to manage pipelines and create reusable patterns to expedite developer productivity. We share the lessons learned during our journey: from basic ETL processes, such as loading data from Amazon RDS to Amazon Redshift, to more sophisticated pipelines to power recommendation engines and search services.

Attendees learn:


	Do's and don’ts of Data Pipeline
	Using Dataduct to streamline your data pipelines
	How to use Data Pipeline to power other data products, such as recommendation systems
	What’s next for Dataduct <a href="javascript:void(0);" class="lessLink">View Less</a></span>


			<small class="length">1 Hour</small>


			<small class="type">Breakout Session</small>


			<span class="track"></span>
				<span class="scheduleStatus">




				</span>
		</div>
		<div class="actionColumn">







		</div>
	</div>


	<div id="session_4907" class="resultRow sessionRow ">
		<div class="detailColumn">



			<a href="sessionDetail.ww?SESSION_ID=4907" class="openInPopup">

				<span class="abbreviation">WRK301 - </span>

				<span class="title">Implementing Twitter Analytics Using Spark Streaming, Scala, and Amazon EMR</span>
			</a>


					<span class="abstract truncatedTxt">Over the course of this workshop, we will launch a Spark Custer and deploy a Spark streaming application written in Scala that analyzes popular tags flowing out of Twitter.&nbsp; Along the way we will learn about AWS EMR, Spark, Spark Streaming, Scala, and how to deploy applications into Spark clusters on AWS EMR.

Prerequisites:


	Participants are expected be familiar with building modest-size applications in Scala.
	Participants should have an AWS account established and available for use during the workshop.&nbsp;
	Please bring your laptop.


Capacity: To encourage the interactive nature of this workshop, the session capacity is limited to approximately 70 attendees.&nbsp; Attendance is based on a first come, first served basis once onsite.&nbsp; Scheduling tools in the session catalog are for planning purposes only.

&nbsp; <a href="javascript:void(0);" class="lessLink">View Less</a></span>


			<small class="length">2 hours</small>


			<small class="type">Workshop</small>


			<span class="track"></span>
				<span class="scheduleStatus">




				</span>
		</div>
		<div class="actionColumn">







		</div>
	</div>


	<div id="session_4909" class="resultRow sessionRow ">
		<div class="detailColumn">



			<a href="sessionDetail.ww?SESSION_ID=4909" class="openInPopup">

				<span class="abbreviation">WRK303 - </span>

				<span class="title">Real-World Data Warehousing with Amazon Redshift and Big Data Solutions from AWS Marketplace</span>
			</a>


					<span class="abstract truncatedTxt">In this workshop, you will work with other attendees as a small team to build an end-to-end&nbsp;data warehouse using Amazon Redshift and&nbsp;by&nbsp;leveraging key AWS Marketplace partners. Your team will learn how to build a data pipeline&nbsp;using an&nbsp;ETL partner from the AWS Marketplace, to&nbsp;perform common validation and aggregation tasks in a&nbsp;data ingestion pipeline. &nbsp;Your team will then learn how to build dashboards and reports using a Data visualization&nbsp;partner&nbsp;from AWS Marketplace, for interactive analysis of large datasets in Amazon Redshift. In less than 2 hours your team will build a fully functional solution to discover meaningful insights from raw-datasets. The session also showcase on how you can extend this solution further to create a near real-time solution by leveraging Amazon Kinesis and other AWS Big Data services.

Prerequisites: Hands-on experience with AWS. Some prior experience with Databases, SQL and familiarity with data-warehousing concepts.

Capacity: To encourage the interactive nature of this workshop, the session capacity is limited to approximately 70 attendees.&nbsp; Attendance is based on a first come, first served basis once onsite.&nbsp; Scheduling tools in the session catalog are for planning purposes only.

&nbsp; <a href="javascript:void(0);" class="lessLink">View Less</a></span>


			<small class="length">2 hours</small>


			<small class="type">Workshop</small>


			<span class="track"></span>
				<span class="scheduleStatus">




				</span>
		</div>
		<div class="actionColumn">







		</div>
	</div>


	<div id="session_4911" class="resultRow sessionRow ">
		<div class="detailColumn">



			<a href="sessionDetail.ww?SESSION_ID=4911" class="openInPopup">

				<span class="abbreviation">WRK304 - </span>

				<span class="title">Build a Recommendation Engine and Use Amazon Machine Learning in Real Time</span>
			</a>


					<span class="abstract truncatedTxt">Build an exciting machine learning model for recommending top restaurants for a customer in real time based on past orders and viewing history. In this guided session you will get hands on with data cleansing, building AML model and doing real time predictions. Dataset will be provided.

Prerequisites:


	Participants should have an AWS account established and available for use during the workshop.&nbsp;
	Participants should bring their own laptop.&nbsp; &nbsp;


Capacity: To encourage the interactive nature of this workshop, the session capacity is limited to approximately 70 attendees.&nbsp; Attendance is based on a first come, first served basis once onsite.&nbsp; Scheduling tools in the session catalog are for planning purposes only. <a href="javascript:void(0);" class="lessLink">View Less</a></span>


			<small class="length">2 hours</small>


			<small class="type">Workshop</small>


			<span class="track"></span>
				<span class="scheduleStatus">




				</span>
		</div>
		<div class="actionColumn">







		</div>
	</div>








</div>